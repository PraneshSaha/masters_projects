{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tag import DefaultTagger  \n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger \n",
    "from nltk.tag import TrigramTagger \n",
    "from nltk.util import ngrams\n",
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the sample file\n",
    "sample_file = pd.read_csv('https://cdn.upgrad.com/UpGrad/temp/9dca5f3b-53c3-47e1-86d5-5ec5dafad6f0/Test_sentences.txt',names=['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test dataset\n",
    "train,test = train_test_split(nltk_data,train_size=0.95,random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_set = [tup for ls in train for tup in ls ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_set = [tup for ls in test for tup in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_word_set,columns=['Word','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(train_word_set,columns=['Word','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pred_df,test=test_df):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of tagging the test dataset\n",
    "    input params\n",
    "    pred_df : dataframe containg the words and predicted tag\n",
    "    test : dataframe containing words and original tags\n",
    "    \n",
    "    returns\n",
    "    dataframe containing words wrongly tagged\n",
    "    \"\"\"\n",
    "    print(len(pred_df),len(test))\n",
    "    test['Tag_pred'] = pred_df['Tag']\n",
    "    df_same = test[test['Tag']==test['Tag_pred']]\n",
    "    print('Number of words in test : ',len(test),' Number of words correctly predicted : ',len(df_same))\n",
    "    print('Accuracy : ',100*len(df_same)/len(test))\n",
    "    return  test[test['Tag']!=test['Tag_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word,tag,train_set=word_df):\n",
    "    tag_df = train_set.groupby(['Word','Tag']).count().reset_index()\n",
    "    word_present = False\n",
    "    if len(train_set[train_set['Word']==word])>0:\n",
    "        word_present = True\n",
    "    word_given_tag = tag_df[tag_df['Word']==word]\n",
    "    \n",
    "    return len(word_given_tag),len(tag_df),word_present\n",
    "\n",
    "def tag_given_tag(tag,train_set=word_df):\n",
    "    \"\"\"\n",
    "    function for calculating the transition probability between two tags\n",
    "    input params\n",
    "    tag : list containing the two tags in order\n",
    "    \n",
    "    returns\n",
    "    counts of occurences of tag1 followed by tag2 and occurence of tag1\n",
    "    \"\"\"\n",
    "    tag2_df = train_set[train_set['Tag']==tag[1]]\n",
    "    tag1_df = train_set[train_set['Tag']==tag[0]]\n",
    "    tag_1_index = np.array(tag1_df.index)+1\n",
    "    tag1_by_2 = tag2_df.loc[tag2_df.index.isin(tag_1_index),:]\n",
    "    return (len(tag1_by_2),len(tag1_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the transition probabilities for all tag combinations and storing in dataframe tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = word_df['Tag'].unique()\n",
    "V = word_df['Word'].unique()\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = tag_given_tag([t1, t2])[0]/tag_given_tag([t1, t2])[1]\n",
    "        \n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, tags_df=tags_df,train_bag=word_df,treat_unknown=False,treatment_func=None):\n",
    "    \"\"\"\n",
    "    Implementation of viterbi algorithm\n",
    "    input params\n",
    "    words : input set of words for which tags have to be predicted\n",
    "    tags_df : dataframe of transition probabilities\n",
    "    train_bag : training dataframe containing word-tag pairs\n",
    "    treat_unknown : whether to treat unknown words seperately; default value False\n",
    "    treatment_func : name of function to treat the unknown words, is required when treat_unknown is True\n",
    "    \n",
    "    returns\n",
    "    list of word and predicted tag pairs\n",
    "    list of whether word was in training set or not, denoted by True and False respectively\n",
    "    \"\"\"\n",
    "    V = set(words)\n",
    "    v = len(V)\n",
    "    word_set = list(train_bag.Word.unique())\n",
    "    state = []\n",
    "    presence = []\n",
    "    Word_count = train_bag.reset_index().groupby(['Word','Tag'])['index'].count().reset_index().rename(\n",
    "        columns={'index':'Count_Word'})\n",
    "    Tag_count = train_bag.reset_index().groupby(['Tag'])['index'].count().reset_index().rename(\n",
    "        columns={'index':'Count_Tag'})\n",
    "    word_tag_merged = Word_count.merge(Tag_count,on='Tag',how='left')\n",
    "    word_tag_merged['Emission_P'] = word_tag_merged['Count_Word']/word_tag_merged['Count_Tag']\n",
    "    word_tag_pivot = pd.pivot_table(word_tag_merged,index='Word',columns='Tag',values='Emission_P').fillna(0)\n",
    "    T = list(word_tag_pivot.loc['.',:].index)\n",
    "    t = len(T)\n",
    "    tags_df_2 = tags_df[T]\n",
    "    for key, word in enumerate(words):\n",
    "        is_present = True\n",
    "        if word in word_set:\n",
    "            emission_p = np.array(word_tag_pivot.loc[word,:])\n",
    "        else:\n",
    "            emission_p = np.array([0]*t)\n",
    "            is_present = False\n",
    "        if key==0:\n",
    "            transition_p = np.array(tags_df_2.loc['.',:])\n",
    "        else:\n",
    "            transition_p = np.array(tags_df_2.loc[state[-1],:])\n",
    "        state_prob = list(np.multiply(transition_p,emission_p))\n",
    "        pmax = max(state_prob)\n",
    "        state_max = T[state_prob.index(pmax)] \n",
    "        state.append(state_max)\n",
    "        presence.append(is_present)\n",
    "\n",
    "    if treat_unknown==True:\n",
    "        _,state = treatment_func(state,words,presence)\n",
    "    return list(zip(words, state)),presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the problem of unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1\n",
    "In the approach 1 we take a probabilistic path to determine the tags of each unknown word. We neglect the emission probabilities as they are all 0.\n",
    "Instead for each missing word we consider 3 cases to calculate the transition probability:\n",
    "1. tag_1, tag_missing, tag_2 : when both succeeding and preceeding words are known we calculate the probability of a particular tag T0 being the tag_missing as\n",
    "               p(T0) = number of occurences of sequence (tag1,T0, tag2)/number of occurences of (tag1,T,tag2) \n",
    "       where T is the set of all available tags\n",
    "2. tag_1, tag_missing : when only preceeding tag is known the expression is same as transition probability,\n",
    "                p(T0) = number of occurences of sequence (tag1, T0)/number of occurence of (tag1, T)\n",
    "3. tag_missing, tag_2 : when only succeeding tag is known, then we follow the same logic\n",
    "                p(T0) = number of occurences of sequence (T0,tag2)/number of occurences if (T,tag2)\n",
    "                \n",
    "For each case we choose the tag giving maximum probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_given_t_one_off(tag,train_set=word_df):\n",
    "    \"\"\"\n",
    "    function for calculating probability of case 1\n",
    "    input param\n",
    "    tag : list of 3 tags in order\n",
    "    \n",
    "    returns\n",
    "    counts as mentioned in above formula for case 1\n",
    "    \"\"\"\n",
    "    tag2_df = train_set[train_set['Tag']==tag[1]]\n",
    "    tag1_df = train_set[train_set['Tag']==tag[0]]\n",
    "    tag3_df = train_set[train_set['Tag']==tag[2]]\n",
    "    tag_1_index = np.array(tag1_df.index)+2\n",
    "    tag1_by_3 = tag3_df.loc[tag3_df.index.isin(tag_1_index),:]\n",
    "    tag_1_by_3_index = np.array(tag1_by_3.index)-1\n",
    "    tag2_bet_1_3 = tag2_df.loc[tag2_df.index.isin(tag_1_by_3_index),:]\n",
    "    return len(tag2_bet_1_3),len(tag1_by_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_1(pos,words,presence):\n",
    "    print('Starting approach 1')\n",
    "    T = list( word_df['Tag'].unique())\n",
    "    df = pd.DataFrame({'Tag':pos,'Word':words,'Present':presence})\n",
    "    index = list(df[df['Present']==False].index)\n",
    "    call_dict = {1:tag_given_tag,2:t_given_t_one_off}\n",
    "    for i in index:\n",
    "        index_prev = 0\n",
    "        index_fol = len(words)-1\n",
    "        if i!=0 and i!=len(words)-1:\n",
    "            index_prev = i-1\n",
    "            index_fol = i+1\n",
    "        prev_pr = int(df.loc[index_prev,'Present'])\n",
    "        fol_pr = int(df.loc[index_fol,'Present'])\n",
    "        is_present = prev_pr + fol_pr\n",
    "        tag = []\n",
    "        if is_present==1:\n",
    "            if prev_pr==1:\n",
    "                tag = ['',df.loc[index_fol,'Tag']]\n",
    "                pos_ch = 0\n",
    "            elif fol_pr==1:\n",
    "                tag = [df.loc[index_prev,'Tag'],'']\n",
    "                pos_ch = 1\n",
    "        elif is_present==2:\n",
    "            tag = [df.loc[index_prev,'Tag'],'',df.loc[index_fol,'Tag']]\n",
    "            pos_ch = 1\n",
    "        else:\n",
    "            continue\n",
    "        prob_mat = []\n",
    "        for t in T:\n",
    "            tag[pos_ch] = t\n",
    "            count_1,count_2 = call_dict[is_present](tag)\n",
    "            prob_mat.append(count_1/count_2)\n",
    "        pmax = max(prob_mat)\n",
    "        state_max = T[prob_mat.index(pmax)]\n",
    "        df.loc[i,'Tag'] = state_max\n",
    "    print('Finished')\n",
    "    return list(df['Word']),list(df['Tag'])\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2\n",
    "For approch 2 we use lexicon based tagging utilising the default, unigram and bigram taggers in NLTK.\n",
    "\n",
    "To model the unknown words we first replace all tags having frquency of occurence 1 in the training set with dummy word 'UNK'. This trains the model to handle unknown words as single word and assign them tags based on their preceeding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = word_df.Word.value_counts().reset_index()\n",
    "low_freq_word = list(word_freq[word_freq['Word']<2]['index'])\n",
    "word_df_2 = word_df.copy()\n",
    "word_df_2['Word'] =  word_df_2.apply(lambda x: 'UNK' if x['Word'] in low_freq_word else x['Word'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approach_2(pos,words,presence,train_word=word_df_2):\n",
    "    print('Starting Approach 2')\n",
    "    df = pd.DataFrame({'Tag':pos,'Word':words,'Present':presence})\n",
    "    df['Word'] = df.apply(lambda x: 'UNK' if ~x['Present'] else x['Word'],axis=1)\n",
    "    index = list(df[df['Present']==False].index)\n",
    "    train_word = [[tuple(ls) for ls in word_df_2.values]]\n",
    "    t0 = nltk.DefaultTagger('NOUN')\n",
    "    t2 = nltk.BigramTagger(train_word, backoff=t0)\n",
    "    t3 = nltk.TrigramTagger(train_word,backoff=t2)\n",
    "    tagged_words = t3.tag(words)\n",
    "    df_tagged = pd.DataFrame(tagged_words,columns=['Word','Tag'])\n",
    "    for i in index:\n",
    "        df.loc[i,'Tag'] = df_tagged.loc[i,'Tag']\n",
    "    df.loc[:,'Word'] = words\n",
    "    print('Finished')\n",
    "    return  list(df['Word']),list(df['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_seq,presence = Viterbi(list(test_df['Word']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting approach 1\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "tagged_seq_2,presence = Viterbi(list(test_df['Word']),treat_unknown=True,treatment_func=approach_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Approach 2\n"
     ]
    }
   ],
   "source": [
    "tagged_seq_3,presence =  Viterbi(list(test_df['Word']),treat_unknown=True,treatment_func=approach_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating tagging accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(tagged_seq,columns=['Word','Tag'])\n",
    "pred_df_2 = pd.DataFrame(tagged_seq_2,columns=['Word','Tag'])\n",
    "pred_df_3 = pd.DataFrame(tagged_seq_3,columns=['Word','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4727 4727\n",
      "Number of words in test :  4727  Number of words correctly predicted :  4268\n",
      "Accuracy :  90.2898244129469\n"
     ]
    }
   ],
   "source": [
    "tagged = get_accuracy(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4727 4727\n",
      "Number of words in test :  4727  Number of words correctly predicted :  4427\n",
      "Accuracy :  93.65348000846203\n"
     ]
    }
   ],
   "source": [
    "tagged_1 = get_accuracy(pred_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4727 4727\n",
      "Number of words in test :  4727  Number of words correctly predicted :  4419\n",
      "Accuracy :  93.48423947535434\n"
     ]
    }
   ],
   "source": [
    "tagged_2 = get_accuracy(pred_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above by treating only the unknown words seperately we get an increase accuracy of more than 3% with both our approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down cases which were incorrectly tagged by original POS tagger and got corrected by your modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n",
      "Starting approach 1\n",
      "Starting Approach 2\n"
     ]
    }
   ],
   "source": [
    "tagged_vit = []\n",
    "tagged_app_1 = []\n",
    "tagged_app_2 = []\n",
    "for i,row in sample_file.iterrows():\n",
    "    word = word_tokenize(row['sent'])\n",
    "    tagged_seq_sample = Viterbi(word)\n",
    "    tagged_seq_sample_1 = Viterbi(word,treat_unknown=True,treatment_func=approach_1)\n",
    "    tagged_seq_sample_2 =  Viterbi(word,treat_unknown=True,treatment_func=approach_2)\n",
    "    tagged_vit.append(tagged_seq_sample)\n",
    "    tagged_app_1.append(tagged_seq_sample_1)\n",
    "    tagged_app_2.append(tagged_seq_sample_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tag_vit = [tup for ls in tagged_vit for tup in ls[0]]\n",
    "presence_vit = [tup for ls in tagged_vit for tup in ls[1]]\n",
    "word_tag_app_1 = [tup[1] for ls in tagged_app_1 for tup in ls[0]]\n",
    "word_tag_app_2 = [tup[1] for ls in tagged_app_2 for tup in ls[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = pd.DataFrame(word_tag_vit,columns=['Word','Tag_Vit'])\n",
    "all_words['Tag_Approach_1'] = word_tag_app_1\n",
    "all_words['Tag_Approach_2'] = word_tag_app_2\n",
    "all_words['Presence'] = presence_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word Tag_Vit Tag_Approach_1 Tag_Approach_2  Presence\n",
      "0     Android       .           NOUN           NOUN     False\n",
      "8      Google       .           NOUN           NOUN     False\n",
      "10    Android       .           NOUN           NOUN     False\n",
      "15         OS       .           NOUN           NOUN     False\n",
      "16  worldwide       .           NOUN           NOUN     False\n"
     ]
    }
   ],
   "source": [
    "print(all_words[all_words['Presence']==False].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see 4 instances in which both approaches correctly identifies the pos tag but the original function fails to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
